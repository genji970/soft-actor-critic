{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPtVsyA_VavI"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import random\n",
        "import collections\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "!pip install gymnasium\n",
        "import gymnasium as gym\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.distributions as distributions\n",
        "from torch.distributions import Normal\n",
        "\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "!pip install omegaconf\n",
        "from omegaconf import OmegaConf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show mujoco\n",
        "!pip install --upgrade gymnasium[mujoco]"
      ],
      "metadata": {
        "id": "P1a7tbXrVeua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('HalfCheetah-v4', render_mode = 'rgb_array')\n",
        "env"
      ],
      "metadata": {
        "id": "DPGMPBcaVewp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "AC_config = OmegaConf.create({\n",
        "    # RL parameter\n",
        "    'gamma': 0.99,\n",
        "\n",
        "    # replay memory\n",
        "    'buffer_limit': int(1e5),\n",
        "    'batch_size': 512,\n",
        "\n",
        "    # neural network parameters\n",
        "    'device': 'cpu',\n",
        "    'hidden_dim': 128,\n",
        "    'state_dim': env.observation_space.shape[0],\n",
        "    'action_dim': int(env.action_space.shape[0]), # cannot use .n because not actions are continuous!\n",
        "\n",
        "    # learning parameters\n",
        "    'lr_actor': 0.0003,\n",
        "})\n",
        ""
      ],
      "metadata": {
        "id": "4F79RcJ6Vey4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# replay buffer\n",
        "class ReplayBuffer():\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.buffer = collections.deque(maxlen=self.config.buffer_limit)\n",
        "\n",
        "    def put(self, transition):\n",
        "        self.buffer.append(transition)\n",
        "\n",
        "    def sample(self, n):\n",
        "\n",
        "        mini_batch = random.sample(self.buffer, n)\n",
        "        s_lst, a_lst, na_lst, r_lst , next_s_lst, done_mask_lst , cnt_list = [], [], [], [], [] , [] ,[]\n",
        "\n",
        "        for transition in mini_batch:\n",
        "            s, a, na , r , next_s, done , cnt = transition\n",
        "            s_lst.append(s.tolist())\n",
        "            a_lst.append(a.tolist())\n",
        "            na_lst.append(na.tolist())\n",
        "            r_lst.append([r])\n",
        "            next_s_lst.append(next_s.tolist())\n",
        "            done_mask = 0.0 if done else 1.0\n",
        "            done_mask_lst.append([done_mask])\n",
        "            cnt_list.append([cnt])\n",
        "\n",
        "        return torch.Tensor(s_lst), torch.Tensor(a_lst), torch.Tensor(na_lst),torch.Tensor(r_lst) , torch.Tensor(next_s_lst), torch.Tensor(done_mask_lst) , torch.Tensor(cnt_list)\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.buffer)"
      ],
      "metadata": {
        "id": "das9VvDAVe0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def soft_update(target, source, tau):\n",
        "  for target_param, param in zip(target.parameters(), source.parameters()):\n",
        "    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)"
      ],
      "metadata": {
        "id": "q6HzfbzMVmtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(nn.Module):\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "    self.data = []\n",
        "    self.config=config\n",
        "    self.tanh = nn.Tanh()\n",
        "    self.relu = nn.ReLU()\n",
        "    self.softmax = nn.Softmax()\n",
        "    self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    self.actor_l1 = nn.Linear(self.config.state_dim, self.config.hidden_dim)\n",
        "    self.actor_mean = nn.Linear(self.config.hidden_dim , self.config.action_dim)\n",
        "    self.actor_std = nn.Linear(self.config.hidden_dim , self.config.action_dim)\n",
        "\n",
        "  def forward(self, batch_num , state):\n",
        "\n",
        "    x = self.actor_l1(state)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    mean_x = self.actor_mean(x)\n",
        "\n",
        "    std_x = self.actor_std(x)\n",
        "    #nan\n",
        "    std_x = self.relu(std_x) + 0.00000001\n",
        "\n",
        "    #torch.clamp x 여러 노가다 중\n",
        "    std_x = torch.where(std_x > 0.3, torch.tensor(0.3), std_x)\n",
        "    std_x = torch.where(std_x <= 0.1, torch.tensor(0.1), std_x)\n",
        "\n",
        "    # DDPG처럼 correlated sample들을 쓸수있는 울벡 noise쓸지, 그냥 soft actor critic으로 쓸지 선택 필요\n",
        "    normal = Normal(mean_x, std_x)\n",
        "    z = normal.rsample()  # reparameterization trick\n",
        "    log_prob = normal.log_prob(z).sum(dim=-1)\n",
        "\n",
        "    action = self.tanh(z)\n",
        "    policy = log_prob\n",
        "\n",
        "    return action , policy"
      ],
      "metadata": {
        "id": "o2yDuctNVmvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Critic(nn.Module):\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "    self.data = []\n",
        "    self.config=config\n",
        "    self.tanh = nn.Tanh()\n",
        "    self.relu = nn.ReLU()\n",
        "    self.swish = nn.SiLU()\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    self.critic_l1 = nn.Linear(self.config.state_dim + self.config.action_dim, self.config.hidden_dim)\n",
        "    self.critic_l2 = nn.Linear(self.config.hidden_dim, 1)\n",
        "\n",
        "  def forward(self, state , action):\n",
        "    x = self.critic_l1(torch.cat([state,action],axis=1))\n",
        "    x = self.tanh(x)\n",
        "    x = self.critic_l2(x)\n",
        "    x = self.tanh(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "yPjAUfNnVmxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actor = Actor(AC_config)\n",
        "critic1 = Critic(AC_config)\n",
        "critic2 = Critic(AC_config)"
      ],
      "metadata": {
        "id": "wnerJBA_VmzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actor_optimizer = torch.optim.Adam(actor.parameters(), lr=AC_config.lr_actor)\n",
        "critic1_optimizer = torch.optim.Adam(critic1.parameters(), lr=AC_config.lr_actor)\n",
        "critic2_optimizer = torch.optim.Adam(critic2.parameters(), lr=AC_config.lr_actor)"
      ],
      "metadata": {
        "id": "qWZsWhGZVm10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epis, epi_rews = 15000, []\n",
        "memory = ReplayBuffer(AC_config)\n",
        "\n",
        "for n_epi in tqdm(range(num_epis)):\n",
        "    state, _ = env.reset()\n",
        "    terminated, truncated = False, False\n",
        "    epi_rew = 0\n",
        "    cnt = 0\n",
        "    while not (terminated or truncated):\n",
        "        cnt += 1\n",
        "        state = torch.Tensor(state)\n",
        "\n",
        "        if n_epi == 0 and cnt == 1:\n",
        "          target_critic = critic1\n",
        "\n",
        "        action , policy = actor(1 , state)\n",
        "        action = action.squeeze(0)\n",
        "\n",
        "        if np.random.rand(1) < 0.001:\n",
        "          action = env.action_space.sample()\n",
        "        else:\n",
        "          action = action.detach().numpy()\n",
        "\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        next_state = torch.Tensor(next_state)\n",
        "\n",
        "        next_action , next_policy = actor(1 , next_state)\n",
        "        next_action = next_action.squeeze(0)\n",
        "\n",
        "        memory.put([state, action, next_action , reward , next_state, terminated or truncated , cnt])\n",
        "\n",
        "        if memory.size() > 4000 and cnt % 2 == 0:\n",
        "          for i in range(5):\n",
        "\n",
        "            states , actions, next_actions, rewards , next_states, dones, times = memory.sample(AC_config.batch_size)\n",
        "\n",
        "            actor_optimizer.zero_grad()\n",
        "            critic1_optimizer.zero_grad()\n",
        "            critic2_optimizer.zero_grad()\n",
        "\n",
        "            action , policy = actor(states.shape[0] , states)\n",
        "            q1 = critic1(states, actions)\n",
        "            q2 = critic2(states, actions)\n",
        "\n",
        "            next_action, next_policy = actor(next_states.shape[0], next_states)\n",
        "\n",
        "            if sum(critic1(next_states,next_actions)) > sum(critic2(next_states, next_actions)):\n",
        "              least_q = critic2\n",
        "            else:\n",
        "              least_q = critic1\n",
        "\n",
        "            a = alpha(states)\n",
        "\n",
        "            soft_update(target_critic , least_q , 0.005)\n",
        "\n",
        "            actor_loss = -least_q(states,actions) - a * policy\n",
        "            critic_loss1 = (q1 - (rewards + AC_config.gamma * target_critic(next_states,next_actions)))**2\n",
        "            critic_loss2 = (q2 - (rewards + AC_config.gamma * target_critic(next_states,next_actions)))**2\n",
        "            alpha_loss = -least_q(states,actions) + torch.relu(-(0.005/AC_config.action_dim) + a * policy)\n",
        "\n",
        "            actor_loss = actor_loss.mean()\n",
        "            critic1_loss = critic_loss1.mean()\n",
        "            critic2_loss = critic_loss2.mean()\n",
        "            alpha_loss = alpha_loss.mean()\n",
        "\n",
        "            actor_loss.backward(retain_graph=True)\n",
        "            critic1_loss.backward(retain_graph=True)\n",
        "            critic2_loss.backward(retain_graph=True)\n",
        "            alpha_loss.backward(retain_graph=True)\n",
        "\n",
        "            actor_optimizer.step()\n",
        "            critic1_optimizer.step()\n",
        "            critic2_optimizer.step()\n",
        "            alpha_optimizer.step()\n",
        "\n",
        "        ## reward formulation\n",
        "        next_state = torch.Tensor(next_state)\n",
        "\n",
        "        # state transition\n",
        "        state = next_state\n",
        "\n",
        "        # record reward\n",
        "        epi_rew += reward\n",
        "\n",
        "    if n_epi % 10 == 0:\n",
        "      plt.figure(figsize=(20, 10))\n",
        "      plt.plot(epi_rews, label='episode returns')\n",
        "      plt.legend(fontsize=20)\n",
        "      plt.show()\n",
        "      plt.close()\n",
        "\n",
        "    epi_rews += [epi_rew]"
      ],
      "metadata": {
        "id": "NuVL2AqDVm3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['MUJOCO_GL']='egl'\n",
        "env = gym.wrappers.RecordVideo(env, video_folder='./videos')"
      ],
      "metadata": {
        "id": "p6e0QXXNVwqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state,_ = env.reset()\n",
        "terminated, truncated = False, False\n",
        "for i in range(1):\n",
        "    while not (terminated or truncated):\n",
        "      state = torch.Tensor(state)\n",
        "      action , policy = actor(state.shape[0] , state)\n",
        "      action = action.detach().numpy()\n",
        "      next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "      state = next_state\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "FbHYEXinVwsi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}